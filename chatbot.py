"""
RAG ç”¨ Markdown â†’ Chroma å–ã‚Šè¾¼ã¿ & Retrieval-QA å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
---------------------------------------------------------------
* Markdown ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒéšå±¤ã® *.md ã‚’å¯¾è±¡
* ãƒ™ã‚¯ãƒˆãƒ« DB ã¯ ./chroma_db ã«æ°¸ç¶šåŒ–ï¼ˆåˆå›ã®ã¿åŸ‹ã‚è¾¼ã¿ï¼‰
* å®Ÿè¡Œä¾‹: python rag_qa.py "èª°ã®æƒ…å ±ã§ã™ã‹"
"""

from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import Iterable

from dotenv import load_dotenv
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.schema import Document
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# ----------------------------------------------------------------------
# â˜… 0. å®šæ•°å®šç¾©
# ----------------------------------------------------------------------
BASE_DIR: Path = Path(__file__).parent
PERSIST_DIR: Path = BASE_DIR / "chroma_db"
MODEL_NAME = "gpt-4o-mini"
TEMPERATURE = 0
HEADERS = [("#", "h1"), ("##", "h2"), ("###", "h3")]  # ãƒãƒ£ãƒ³ã‚¯å˜ä½

# ----------------------------------------------------------------------
# â˜… 1. API ã‚­ãƒ¼èª­ã¿è¾¼ã¿
# ----------------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise EnvironmentError(
        "OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚ .env ã‚‚ã—ãã¯ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚"
    )


# ----------------------------------------------------------------------
# â˜… 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ â†’ ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------------------------------------------------
def load_markdown_chunks(files: Iterable[Path]) -> list[Document]:
    """
    Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦è¿”ã™ã€‚
    """
    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=HEADERS)
    chunks: list[Document] = []

    for md_file in files:
        doc = UnstructuredMarkdownLoader(md_file).load()[0]  # 1 ãƒ•ã‚¡ã‚¤ãƒ« = 1 Document
        for chunk in splitter.split_text(doc.page_content):
            chunk.metadata = {"source": str(md_file)}  # Chroma ç”¨ (str å‹å¿…é ˆ)
            chunks.append(chunk)

    return chunks


# ----------------------------------------------------------------------
# â˜… 3. ãƒ™ã‚¯ãƒˆãƒ« DB æ§‹ç¯‰ / å†åˆ©ç”¨
# ----------------------------------------------------------------------


def get_vectordb(chunks: list[Document]) -> Chroma:
    """
    * æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã£ã¦ã‚‚ **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ 0 ä»¶ãªã‚‰å†æ§‹ç¯‰**
    * æ—¢å­˜ DB ã« â€œæ–°ã—ã„ sourceâ€ ãŒã‚ã‚Œã° **å·®åˆ†ã ã‘ add_documents()**
      - ã“ã“ã§ã¯ metadata["source"] å˜ä½ã§é‡è¤‡åˆ¤å®š
    * ä½•ã‚‚è¿½åŠ ãŒç„¡ã‘ã‚Œã°ãã®ã¾ã¾å†åˆ©ç”¨
    """
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

    # --- â‘  æ—¢å­˜ DB ãŒã‚ã‚‹ã‹åˆ¤å®š ---------------------------------
    if PERSIST_DIR.exists():
        vectordb = Chroma(
            persist_directory=str(PERSIST_DIR), embedding_function=embeddings
        )

        total = vectordb._collection.count()
        if total == 0:
            print("âš ï¸  æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¯ã‚ã‚‹ãŒãƒ¬ã‚³ãƒ¼ãƒ‰ 0 ä»¶ â†’ å†æ§‹ç¯‰ã—ã¾ã™")
            vectordb.delete_collection()  # ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç©ºã«
        else:
            # --- â‘¡ å·®åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆsource ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§åˆ¤å®šï¼‰ ----------
            existing = {
                meta["source"]
                for meta in vectordb._collection.get(include=["metadatas"])["metadatas"]
            }

            new_chunks = [c for c in chunks if c.metadata["source"] not in existing]
            if new_chunks:
                print(f"ğŸ”„  æ–°è¦ {len(new_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ")
                vectordb.add_documents(new_chunks)
                vectordb.persist()
            else:
                print("âœ…  æ—¢å­˜ Chroma DB ã‚’ãã®ã¾ã¾å†åˆ©ç”¨ã—ã¾ã™")
            return vectordb

    # --- â‘¢ ã“ã“ã«æ¥ãŸã‚‰ã€Œãƒ•ã‚©ãƒ«ãƒ€ç„¡ã—ã€ã¾ãŸã¯ã€Œå†æ§‹ç¯‰ã€ ------------
    print("ğŸ†•  Chroma DB ã‚’æ–°è¦æ§‹ç¯‰ã—ã¦ã„ã¾ã™â€¦")
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=str(PERSIST_DIR),
    )
    vectordb.persist()
    print(f"âœ…  Chroma DB ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆ{len(chunks)} ãƒãƒ£ãƒ³ã‚¯ï¼‰")
    return vectordb


# ----------------------------------------------------------------------
# â˜… 4. Retrieval-QA å®Ÿè¡Œ
# ----------------------------------------------------------------------
def run_query(vectordb: Chroma, query: str) -> None:
    llm = ChatOpenAI(
        model_name=MODEL_NAME, temperature=TEMPERATURE, api_key=OPENAI_API_KEY
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(),
        return_source_documents=True,
    )

    result = qa(query)
    print(f"\nQ: {query}\nA: {result['result']}\n")
    print("--- å¼•ç”¨å…ƒãƒãƒ£ãƒ³ã‚¯ ---")
    for doc in result["source_documents"]:
        snippet = doc.page_content.replace("\n", " ")[:120]
        print(f"[{doc.metadata['source']}] {snippet} â€¦")


# ----------------------------------------------------------------------
# â˜… 5. ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ----------------------------------------------------------------------
def main() -> None:
    # Markdown ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
    md_files = list(BASE_DIR.glob("*.md"))
    if not md_files:
        raise FileNotFoundError(f"Markdown (*.md) ãŒ {BASE_DIR} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ãƒ™ã‚¯ãƒˆãƒ« DB ç”¨ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆï¼ˆåˆå›ã®ã¿å®Ÿè³ªã‚³ã‚¹ãƒˆï¼‰
    chunks = load_markdown_chunks(md_files)

    # ãƒ™ã‚¯ãƒˆãƒ« DB æº–å‚™
    vectordb = get_vectordb(chunks)

    # ã‚¯ã‚¨ãƒªã¯ CLI å¼•æ•° or ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    query = sys.argv[1] if len(sys.argv) > 1 else "èª°ã®æƒ…å ±ã§ã™ã‹"
    run_query(vectordb, query)


if __name__ == "__main__":
    main()
